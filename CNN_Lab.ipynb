{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Image Classification Laboration\n",
    "\n",
    "\n",
    "Images used in this laboration are from CIFAR 10 (https://en.wikipedia.org/wiki/CIFAR-10). The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. Your task is to make a classifier, using a convolutional neural network, that can correctly classify each image into the correct class.\n",
    "\n",
    "You need to answer all questions in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What is a convolution\n",
    "\n",
    "To understand a bit more about convolutions, we will first test the convolution function in scipy using a number of classical filters. \n",
    "\n",
    "Convolve the image with Gaussian filter, a Sobel X filter, and a Sobel Y filter, using the function 'convolve2d' in 'signal' from scipy.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html\n",
    "\n",
    "In a CNN, many filters are applied in each layer, and the filter coefficients are learned through back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import numpy as np\n",
    "\n",
    "# Get a test image\n",
    "from scipy import misc\n",
    "image = misc.ascent()\n",
    "\n",
    "# Define a help function for creating a Gaussian filter\n",
    "def matlab_style_gauss2D(shape=(3,3),sigma=0.5):\n",
    "    \"\"\"\n",
    "    2D gaussian mask - should give the same result as MATLAB's\n",
    "    fspecial('gaussian',[shape],[sigma])\n",
    "    \"\"\"\n",
    "    m,n = [(ss-1.)/2. for ss in shape]\n",
    "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
    "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
    "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    return h\n",
    "\n",
    "# Create Gaussian filter with certain size and standard deviation\n",
    "gaussFilter = matlab_style_gauss2D((15,15),4)\n",
    "\n",
    "# Define filter kernels for SobelX and Sobely\n",
    "sobelX = np.array([[ 1, 0,  -1],\n",
    "                    [2, 0, -2],\n",
    "                    [1, 0, -1]]) \n",
    "\n",
    "sobelY = np.array([[ 1, 2,  1],\n",
    "                    [0, 0, 0],\n",
    "                    [-1, -2, -1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform convolution using the function 'convolve2d' for the different filters\n",
    "filterResponseGauss  = signal.convolve2d(image, gaussFilter, mode='valid')\n",
    "filterResponseSobelX = signal.convolve2d(image, sobelX, mode='valid')\n",
    "filterResponseSobelY = signal.convolve2d(image, sobelY, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian filter with certain size and standard deviation\n",
    "gaussFilter = matlab_style_gauss2D((15,15),4)\n",
    "\n",
    "# Define filter kernels for SobelX and Sobely\n",
    "sobelX = np.array([[ 1, 0,  -1],\n",
    "                    [2, 0, -2],\n",
    "                    [1, 0, -1]]) \n",
    "\n",
    "sobelY = np.array([[ 1, 2,  1],\n",
    "                    [0, 0, 0],\n",
    "                    [-1, -2, -1]]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show filter responses\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax_orig, ax_filt1, ax_filt2, ax_filt3) = plt.subplots(1, 4, figsize=(20, 6))\n",
    "ax_orig.imshow(image, cmap='gray')\n",
    "ax_orig.set_title('Original')\n",
    "ax_orig.set_axis_off()\n",
    "ax_filt1.imshow(np.absolute(filterResponseGauss), cmap='gray')\n",
    "ax_filt1.set_title('Filter response')\n",
    "ax_filt1.set_axis_off()\n",
    "ax_filt2.imshow(np.absolute(filterResponseSobelX), cmap='gray')\n",
    "ax_filt2.set_title('Filter response')\n",
    "ax_filt2.set_axis_off()\n",
    "ax_filt3.imshow(np.absolute(filterResponseSobelY), cmap='gray')\n",
    "ax_filt3.set_title('Filter response')\n",
    "ax_filt3.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 2: Understanding convolutions\n",
    "\n",
    "Question 1: What do the 3 different filters (Gaussian, SobelX, SobelY) do to the original image?\n",
    "\n",
    "Question 2: What is the size of the original image? How many channels does it have? How many channels does a color image normally have?\n",
    "\n",
    "Question 3: What is the size of the different filters?\n",
    "\n",
    "Question 4: What is the size of the filter response if mode 'same' is used for the convolution ?\n",
    "\n",
    "Question 5: What is the size of the filter response if mode 'valid' is used for the convolution? How does the size of the valid filter response depend on the size of the filter? \n",
    "\n",
    "Question 6: Why are 'valid' convolutions a problem for CNNs with many layers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for checking sizes of image and filter responses\n",
    "print(\"Image size : {}\".format(image.shape))\n",
    "print(\"\\nFilter response size in 'valid' mode :\" +\n",
    "      \"\\n\\tGaussian filter response : {}\".format(filterResponseGauss.shape) +\n",
    "      \"\\n\\tSobelX / SobelY filter response : {}\".format(filterResponseSobelX.shape))\n",
    "\n",
    "filterResponseGauss2  = signal.convolve2d(image, gaussFilter, mode='same')\n",
    "filterResponseSobelX2 = signal.convolve2d(image, sobelX, mode='same')\n",
    "filterResponseSobelY2 = signal.convolve2d(image, sobelY, mode='same')\n",
    "\n",
    "print(\"\\nFilter response size in 'same' mode :\" +\n",
    "      \"\\n\\tGaussian filter response : {}\".format(filterResponseGauss2.shape) +\n",
    "      \"\\n\\tSobelX / SobelY filter response : {}\".format(filterResponseSobelX2.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers Q1-Q6\n",
    "\n",
    "`Q1:` Gaussian filter blurs the image and reduces contrast. The sobelX and sobelY filters detect edges in the vertical and horizontal directions respectively.\n",
    "\n",
    "`Q2:` Original image is of size 512 x 512. Since all the images in the dataset are color images, the original image has 3 channels. Any color image normally has 3 channels, 1 for each of the RGB colors.\n",
    "\n",
    "`Q3:` Gaussian filter is of size 15 x 15, sobelX and sobelY filters are both of size 3 x 3.\n",
    "\n",
    "`Q4:` The gaussian, sobelX and sobelY filter responses all have the same size as that of the original image when mode 'same' is used for convolution. \n",
    "\n",
    "`Q5:` The gaussian filter of size 15 x 15 gives a response of size 498 x 498 and the sobelX/Y filters of size 3 x 3 give response of size 510 x 510 when mode 'valid' is used for convolution on an image of size 512 x 512. The valid convolution gives output only when the filter completely overlaps with the image and we will get a full overlap only when the center of the filter is atleast $(f-1)/2$ (for odd filter size $f$) away from the top/bottom and right/left edges of the image. So the size of the response will be $i-(f-1) = i-f+1$ where $i$ is the image size and $f$ is the odd filter size.\n",
    "\n",
    "`Q6:` From above we know that a valid convolution reduces the size of the input image. In a CNN with many layers, the output from one valid convolution will be fed as an input for the next convolution and the size of the output will go on diminishing. With many layers and hence repeated convolutions, we might run out of pixels in the original image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 3: Get a graphics card\n",
    "\n",
    "Let's make sure that our script can see the graphics card that will be used. The graphics cards will perform all the time consuming convolutions in every training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarning from numpy\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";\n",
    "\n",
    "# Allow growth of GPU memory, otherwise it will always look like all the memory is being used\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 4: How fast is the graphics card?\n",
    "\n",
    "Lets investigate how much faster a convolution is with the graphics card\n",
    "\n",
    "Question 7: Why are the filters of size 7 x 7 x 3, and not 7 x 7 ? \n",
    "\n",
    "Question 8: What operation is performed by the 'Conv2D' layer? Is it a standard 2D convolution, as performed by the function signal.convolve2d we just tested?\n",
    "\n",
    "Question 9: How much faster is the graphics card, compared to the CPU, for convolving a batch of 100 images?\n",
    "\n",
    "Question 10: How much faster is the graphics card, compared to the CPU, for convolving a batch of 2 images? Explain the difference compared to 100 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers Q7-Q10\n",
    "\n",
    "`Q7:` The filters are of size 7 x 7 x 3 because the input image has three color (RGB) channels and the filter operates seperately on each of the input channels. The filter is a collection of kernels, one for every input channel to the layer. The kernel outputs for each channel are summed to produce one overall output channel\n",
    "\n",
    "`Q8:` The Conv2D layer performs a spatial convolution over 2D images. It is the same operation as performed by the function signal.convolve2d i.e. it is a linear combination of series of 2D image data with a few coefficients or weights. \n",
    "\n",
    "`Q9:` CPU=0.993915017000063, GPU=0.19508830099994157, GPU speedup over CPU = 5x. The GPU speedup over CPU is quite good in this case. \n",
    "\n",
    "`Q10:` CPU=0.06825254199975461, GPU=0.05537720200027252, GPU speedup over CPU = 1x. The GPU does not give any significant performance improvement over CPU when the batch size=2. This is because when the batch size is too small, even though we are utilizing the parallelization capabilities of the GPU, the no of iterations in each epoch increases which increases the training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to compare processing time of CPU and GPU\n",
    "\n",
    "import timeit\n",
    "\n",
    "n_images_in_batch = 1\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "# Perform convolutions using the CPU\n",
    "def cpu():\n",
    "  with tf.device('/cpu:0'):\n",
    "    random_images = tf.random.normal((n_images_in_batch, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_images)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "# Perform convolutions using the GPU (graphics card)\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    random_images = tf.random.normal((n_images_in_batch, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_images)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "  \n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the convolution several times and measure the time\n",
    "print('Time (s) to convolve 32 filters of size 7 x 7 x 3 over 100 random images of size 100 x 100 x 3'\n",
    "      ' (batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 5:  Load data\n",
    "Time to make a 2D CNN. Load the images and labels from keras.datasets, this cell is already finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Download CIFAR train and test data\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = cifar10.load_data()\n",
    "\n",
    "print(\"Training images have size {} and labels have size {} \".format(Xtrain.shape, Ytrain.shape))\n",
    "print(\"Test images have size {} and labels have size {} \\n \".format(Xtest.shape, Ytest.shape))\n",
    "\n",
    "# Reduce the number of images for training and testing to 10000 and 2000 respectively, \n",
    "# to reduce processing time for this laboration\n",
    "Xtrain = Xtrain[0:10000]\n",
    "Ytrain = Ytrain[0:10000]\n",
    "\n",
    "Xtest = Xtest[0:2000]\n",
    "Ytest = Ytest[0:2000]\n",
    "\n",
    "Ytestint = Ytest\n",
    "\n",
    "print(\"Reduced training images have size %s and labels have size %s \" % (Xtrain.shape, Ytrain.shape))\n",
    "print(\"Reduced test images have size %s and labels have size %s \\n\" % (Xtest.shape, Ytest.shape))\n",
    "\n",
    "# Check that we have some training examples from each class\n",
    "for i in range(10):\n",
    "    print(\"Number of training examples for class {} is {}\" .format(i,np.sum(Ytrain == i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cifar10.load_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 6: Plotting\n",
    "\n",
    "Lets look at some of the training examples, this cell is already finished. You will see different examples every time you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "for i in range(18):\n",
    "    idx = np.random.randint(7500)\n",
    "    label = Ytrain[idx,0]\n",
    "    \n",
    "    plt.subplot(3,6,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(Xtrain[idx])\n",
    "    plt.title(\"Class: {} ({})\".format(label, classes[label]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Part 7: Split data into training, validation and testing\n",
    "Split your training data into training (Xtrain, Ytrain) and validation (Xval, Yval), so that we have training, validation and test datasets (as in the previous laboration). We use a function in scikit learn. Use 25% of the data for validation.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split training data into training and validation datasets\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(Xtrain, Ytrain, test_size=0.25)\n",
    "\n",
    "# Print the size of training data, validation data and test data\n",
    "print(\"Training images have size %s and labels have size %s \" % (Xtrain.shape, Ytrain.shape))\n",
    "print(\"Validation images have size %s and labels have size %s\" % (Xval.shape, Yval.shape))\n",
    "print(\"Test images have size %s and labels have size %s \\n\" % (Xtest.shape, Ytest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 8: Preprocessing of images\n",
    "\n",
    "Lets perform some preprocessing. The images are stored as uint8, i.e. 8 bit unsigned integers, but need to be converted to 32 bit floats. We also make sure that the range is -1 to 1, instead of 0 - 255. This cell is already finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datatype for Xtrain, Xval, Xtest, to float32\n",
    "Xtrain = Xtrain.astype('float32')\n",
    "Xval = Xval.astype('float32')\n",
    "Xtest = Xtest.astype('float32')\n",
    "\n",
    "# Change range of pixel values to [-1,1]\n",
    "Xtrain = Xtrain / 127.5 - 1\n",
    "Xval = Xval / 127.5 - 1\n",
    "Xtest = Xtest / 127.5 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 9: Preprocessing of labels\n",
    "\n",
    "The labels (Y) need to be converted from e.g. '4' to \"hot encoded\", i.e. to a vector of type [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] . We use a function in Keras, see https://keras.io/utils/#to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Print shapes before converting the labels\n",
    "print(\"Training image labels have size {}\".format(Ytrain.shape))\n",
    "print(\"Validation image labels have size {}\".format(Yval.shape))\n",
    "print(\"Test image labels have size {}\".format(Ytest.shape))\n",
    "\n",
    "# Your code for converting Ytrain, Yval, Ytest to categorical\n",
    "Ytrain = to_categorical(y=Ytrain, num_classes=10)\n",
    "Yval   = to_categorical(y=Yval)\n",
    "Ytest  = to_categorical(y=Ytest)\n",
    "\n",
    "# Print shapes after converting the labels\n",
    "print(\"\\nTraining image labels have size {}\".format(Ytrain.shape))\n",
    "print(\"Validation image labels have size {}\".format(Yval.shape))\n",
    "print(\"Test image labels have size {}\".format(Ytest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 10: 2D CNN\n",
    "Finish this code to create the image classifier, using a 2D CNN. Each convolutional layer will contain 2D convolution, batch normalization and max pooling. After the convolutional layers comes a flatten layer and a number of intermediate dense layers. The convolutional layers should take the number of filters as an argument, use a kernel size of 3 x 3, 'same' padding, and relu activation functions. The number of filters will double with each convolutional layer. The max pooling layers should have a pool size of 2 x 2. The intermediate dense layers before the final dense layer should take the number of nodes as an argument, use relu activation functions, and be followed by batch normalization. The final dense layer should have 10 nodes (= the number of classes in this laboration) and 'softmax' activation. Here we start with the Adam optimizer.\n",
    "\n",
    "Relevant functions are\n",
    "\n",
    "`model.add()`, adds a layer to the network\n",
    "\n",
    "`Dense()`, a dense network layer\n",
    "\n",
    "`Conv2D()`, performs 2D convolutions with a number of filters with a certain size (e.g. 3 x 3). \n",
    "\n",
    "`BatchNormalization()`, perform batch normalization\n",
    "\n",
    "`MaxPooling2D()`, saves the max for a given pool size, results in down sampling\n",
    "\n",
    "`Flatten()`, flatten a multi-channel tensor into a long vector\n",
    "\n",
    "`model.compile()`, compile the model, add \" metrics=['accuracy'] \" to print the classification accuracy during the training\n",
    "\n",
    "See https://keras.io/layers/core/ for information on how the `Dense()` and `Flatten()` functions work\n",
    "\n",
    "See https://keras.io/layers/convolutional/ for information on how `Conv2D()` works\n",
    "\n",
    "See https://keras.io/layers/pooling/ for information on how `MaxPooling2D()` works\n",
    "\n",
    "Import a relevant cost function for multi-class classification from keras.losses (https://keras.io/losses/)\n",
    "\n",
    "See https://keras.io/models/model/ for how to compile, train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy as CC\n",
    "\n",
    "# Set seed from random number generator, for better comparisons\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "\n",
    "def build_CNN(input_shape, n_conv_layers=2, n_filters=16, n_dense_layers=0, n_nodes=50, use_dropout=False, learning_rate=0.01):\n",
    "\n",
    "    # Setup a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add first convolutional layer to the model, requires input shape\n",
    "    model.add(Conv2D(filters=n_filters, kernel_size=(3,3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "    # Add remaining convolutional layers to the model, the number of filters should increase a factor 2 for each layer\n",
    "    for i in range(n_conv_layers-1):\n",
    "        n_filters = n_filters*2\n",
    "        model.add(Conv2D(filters=n_filters, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "                         \n",
    "    # Add flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Add intermediate dense layers\n",
    "    for i in range(n_dense_layers):\n",
    "        model.add(Dense(units=n_nodes, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        if use_dropout==True:\n",
    "            model.add(Dropout(rate=0.5))\n",
    "        \n",
    "    # Add final dense layer\n",
    "    model.add(Dense(units=10, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=CC, optimizer=Adam(learning_rate=learning_rate), metrics=[\"categorical_accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a help function for plotting the training results\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_results(history):\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    acc = history.history['categorical_accuracy']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_acc = history.history['val_categorical_accuracy']\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['Training','Validation'])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.legend(['Training','Validation'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Train 2D CNN\n",
    "\n",
    "Time to train the 2D CNN, start with 2 convolutional layers, no intermediate dense layers, learning rate = 0.01. The first convolutional layer should have 16 filters (which means that the second convolutional layer will have 32 filters).\n",
    "\n",
    "Relevant functions\n",
    "\n",
    "`build_CNN`, the function we defined in Part 10, call it with the parameters you want to use\n",
    "\n",
    "`model.fit()`, train the model with some training data\n",
    "\n",
    "`model.evaluate()`, apply the trained model to some test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 convolutional layers, no intermediate dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Build model\n",
    "model1 = build_CNN(input_shape=input_shape, n_conv_layers=2, n_filters=16, n_dense_layers=0, learning_rate=0.01)\n",
    "\n",
    "# Train the model using training data and validation data\n",
    "history1 = model1.fit(x=Xtrain, y=Ytrain, batch_size=batch_size, epochs=epochs, validation_data=(Xval, Yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on test set, not used in training or validation\n",
    "score = model1.evaluate(x=Xtest, y=Ytest, batch_size=batch_size)\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Improving performance\n",
    "Write down the test accuracy, are you satisfied with the classifier performance (random chance is 10%) ? \n",
    "\n",
    "Question 11: How big is the difference between training and test accuracy?\n",
    "\n",
    "Question 12: How busy is the GPU for a batch size of 100? How much GPU memory is used? Hint: run 'watch nvidia-smi' on the cloud computer during training. \n",
    "\n",
    "Question 13: For the DNN laboration we used a batch size of 10,000, why do we need to use a smaller batch size in this laboration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers Q11-Q13\n",
    "\n",
    "`Q11:` We achieved a training accuracy of 0.94 and test accuracy of 0.53. The test accuracy is lower than the training by 0.41 i.e. almost by half. It can be said that the model has an acceptable classification accuracy considering 10% as random chance. \n",
    "\n",
    "`Q12:` \n",
    "\n",
    "`Q13:` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 convolutional layers, 1 intermediate dense layer (50 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Build model\n",
    "model2 = build_CNN(input_shape=input_shape, n_conv_layers=2, n_filters=16, n_dense_layers=1, n_nodes=50, learning_rate=0.01)\n",
    "\n",
    "# Train the model  using training data and validation data\n",
    "history2 = model2.fit(x=Xtrain, y=Ytrain, batch_size=batch_size, epochs=epochs, validation_data=(Xval, Yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on test set, not used in training or validation\n",
    "score = model2.evaluate(x=Xtest, y=Ytest, batch_size=batch_size)\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 convolutional layers, 1 intermediate dense layer (50 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Build model\n",
    "model3 = build_CNN(input_shape=input_shape, n_conv_layers=4, n_filters=16, n_dense_layers=1, n_nodes=50, learning_rate=0.01)\n",
    "\n",
    "# Train the model  using training data and validation data\n",
    "history3 = model3.fit(x=Xtrain, y=Ytrain, batch_size=batch_size, epochs=epochs, validation_data=(Xval, Yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on test set, not used in training or validation\n",
    "score = model3.evaluate(x=Xtest, y=Ytest, batch_size=batch_size)\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 13: Plot the CNN architecture\n",
    "\n",
    "To understand your network better, print the architecture using `model.summary()`\n",
    "\n",
    "Question 14: How many trainable parameters does your network have? Which part of the network contains most of the parameters?\n",
    "\n",
    "Question 15: What is the input to and output of a Conv2D layer? What are the dimensions of the input and output? \n",
    "\n",
    "Question 16: Is the batch size always the first dimension of each 4D tensor? Check the documentation for Conv2D, https://keras.io/layers/convolutional/\n",
    "\n",
    "Question 17: If a convolutional layer that contains 128 filters is applied to an input with 32 channels, what is the number of channels in the output?\n",
    "\n",
    "Question 18: Why is the number of parameters in each Conv2D layer *not* equal to the number of filters times the number of filter coefficients per filter (plus biases)?\n",
    "\n",
    "Question 19: How does MaxPooling help in reducing the number of parameters to train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers Q14-Q19\n",
    "\n",
    "`Q14:` The CNN has a total of 200,500 trainable parameters. The last convolutional layer contains the most number of parameters. The last Conv2D layer has 128 filters of size 3 x 3 and hence 3 x 3 x 64 weights (64 is the number of input channels to the last Conv2D layer) and 1 bias for each filter. Therefore, this layer has ((3 x 3 x 64) + 1) x 128 = 73856 parameters in total.  \n",
    "\n",
    "The dense layer contains the next most number of parameters as every neuron is connected to every other neuron from the previous layer. The fully connected layer has 50 nodes each connected to all of the 512 nodes from the flattened layer. Hence it has 50 x 512 weights and 50 biases = 25650 total parameters.\n",
    "\n",
    "`Q15:` The input to a Conv2D layer is an input image with channels and the output is a convolved image with (usually higher) output channels. These convolved image channels are called feature maps. Each output channel (feature map) is the output of one filter applied to the previous layer. If the input dimensions to a conv2D layer with $n_{filter}$ filters are $i_{width}$ x $i_{height}$ x $i_{channels}$ then the output of the layer has dimenions $i_{width}$ x $i_{height}$ x $n_{filter}$.\n",
    "\n",
    "`Q16:` Yes, the first dimension of a 4D tensor is always the batch_size. It does not depend on the data_format parameter of the Conv2D layer. If the data_format = channels_last, then the dimension of the 4D tensor is (batch_size, rows, cols, channels) and if the data_format = channels_first, then the dimension of the 4D tensor is (batch_size, channels, rows, cols). \n",
    "\n",
    "`Q17:` The number of output channels is the same as the number of filters in the convolutional layer. Each of the 128 filters in the Conv2D layer has 32 filter kernels (equal to the number of input channels). So we will get one output channel for each filter which is the sum of all the 32 kernels in that filter convolved with the 32 input channels respectively. \n",
    "\n",
    "`Q18:` The number of filter coefficients per filter is $f_{width}$ x $f_{height}$ but each filter has depth (filter kernels) equal to the number of input channels i.e. $f_{kernels}$=$i_{channels}$. The filter output is the sum over all the kernel outputs. Hence the number of parameters in a Conv2D layer = (($f_{width}$ x $f_{height}$ x $f_{kernels}$ + 1) x $n_{filter}$ where $f_{kernels}$=$i_{channels}$. The 1 is for a bias per filter.   \n",
    "\n",
    "`Q19:` MaxPooling layer with pool size=2 reduces the output dimension by half of the input dimension. For every 2 x 2 neighbourhood in the input image, we take the maximum value as the output. The default stride is equal to the pool size. This reduces the output dimension and hence the parameters associated with the successive layers in the network. MaxPooling itself doesn't have any learnable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print network architecture\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 14: Dropout regularization\n",
    "\n",
    "Add dropout regularization to each intermediate dense layer, dropout probability 50%.\n",
    "\n",
    "Question 20: How much did the test accuracy improve with dropout, compared to without dropout?\n",
    "\n",
    "Question 21: What other types of regularization can be applied? How can you add L2 regularization for the convolutional layers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers Q20-Q21\n",
    "\n",
    "`Q20:` The test accuracy of the model increased approximately by 0.1 with dropout regularization. The test accuracy of the CNN model without dropout is 0.558 and with dropout is 0.565 \n",
    "\n",
    "`Q21:` Bias and kernel regularization, or image augmentation can also be applied to the CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 convolutional layers, 1 intermediate dense layer (50 nodes), dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Build model\n",
    "model4 = build_CNN(input_shape=input_shape, n_conv_layers=4, n_filters=16, n_dense_layers=1, n_nodes=50, \n",
    "                   use_dropout=True, learning_rate=0.01)\n",
    "\n",
    "# Train the model  using training data and validation data\n",
    "history4 = model4.fit(x=Xtrain, y=Ytrain, batch_size=batch_size, epochs=epochs, validation_data=(Xval, Yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on test set, not used in training or validation\n",
    "score = model4.evaluate(x=Xtest, y=Ytest, batch_size=batch_size)\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 15: Tweaking performance\n",
    "\n",
    "You have now seen the basic building blocks of a 2D CNN. To further improve performance involves changing the number of convolutional layers, the number of filters per layer, the number of intermediate dense layers, the number of nodes in the intermediate dense layers, batch size, learning rate, number of epochs, etc. Spend some time (30 - 90 minutes) testing different settings.\n",
    "\n",
    "Question 22: How high test accuracy can you obtain? What is your best configuration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Q22\n",
    "\n",
    "`Q22:` We are able to get an accuracy of upto 0.62 on test images with the below configuration:    \n",
    "\n",
    "convolutional layers: 4   \n",
    "max pool layers: 4   \n",
    "intermediate dense layer: 1  \n",
    "number of nodes in dense layer: 80  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your best config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Build model\n",
    "model5 = build_CNN(input_shape=input_shape, n_conv_layers=4, n_filters=16, n_dense_layers=1, n_nodes=80, \n",
    "                   use_dropout=True, learning_rate=0.01)\n",
    "\n",
    "# Train the model  using training data and validation data\n",
    "history5 = model5.fit(x=Xtrain, y=Ytrain, batch_size=batch_size, epochs=epochs, validation_data=(Xval, Yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on test set, not used in training or validation\n",
    "score = model5.evaluate(x=Xtest, y=Ytest, batch_size=batch_size)\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 16: Rotate the test images\n",
    "\n",
    "How high is the test accuracy if we rotate the test images? In other words, how good is the CNN at generalizing to rotated images?\n",
    "\n",
    "Rotate each test image 90 degrees, the cells are already finished.\n",
    "\n",
    "Question 23: What is the test accuracy for rotated test images, compared to test images without rotation? Explain the difference in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Q23\n",
    "\n",
    "`Q23:` The test accuracy of the model for rotated images is 0.23 which is quite low. It is not very better than classifying images at random chance. The CNN model is trained to look for the learned features in only a certain orientations. This is why it results in a low test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myrotate(images):\n",
    "\n",
    "    images_rot = np.rot90(images, axes=(1,2))\n",
    "    \n",
    "    return images_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the test images 90 degrees\n",
    "Xtest_rotated = myrotate(Xtest)\n",
    "\n",
    "# Look at some rotated images\n",
    "plt.figure(figsize=(16,4))\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(500)\n",
    "    \n",
    "    plt.subplot(2,10,i+1)\n",
    "    plt.imshow(Xtest[idx]/2+0.5)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2,10,i+11)\n",
    "    plt.imshow(Xtest_rotated[idx]/2+0.5)\n",
    "    plt.title(\"Rotated\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on rotated test set\n",
    "score = model5.evaluate(x=Xtest_rotated, y=Ytest, batch_size=batch_size)\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 17: Augmentation using Keras `ImageDataGenerator`\n",
    "\n",
    "We can increase the number of training images through data augmentation (we now ignore that CIFAR10 actually has 60 000 training images). Image augmentation is about creating similar images, by performing operations such as rotation, scaling, elastic deformations and flipping of existing images. This will prevent overfitting, especially if all the training images are in a certain orientation.\n",
    "\n",
    "We will perform the augmentation on the fly, using a built-in function in Keras, called `ImageDataGenerator`\n",
    "\n",
    "See https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all 60 000 training images again. ImageDataGenerator manages validation data on its own\n",
    "(Xtrain, Ytrain), _ = cifar10.load_data()\n",
    "\n",
    "# Reduce number of images to 10,000\n",
    "Xtrain = Xtrain[0:10000]\n",
    "Ytrain = Ytrain[0:10000]\n",
    "\n",
    "# Change data type and rescale range\n",
    "Xtrain = Xtrain.astype('float32')\n",
    "Xtrain = Xtrain / 127.5 - 1\n",
    "\n",
    "# Convert labels to hot encoding\n",
    "Ytrain = to_categorical(Ytrain, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a data generator with on-the-fly data augmentation, 20% validation split\n",
    "# Use a rotation range of 30 degrees, horizontal and vertical flipping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=30, horizontal_flip=True, vertical_flip=True, validation_split=0.2)\n",
    "\n",
    "# Setup a flow for training data, assume that we can fit all images into CPU memory\n",
    "datagen_train = datagen.flow(x=Xtrain, y=Ytrain, batch_size=len(Xtrain)*0.8, subset=\"training\")\n",
    "\n",
    "# Setup a flow for validation data, assume that we can fit all images into CPU memory\n",
    "datagen_valid = datagen.flow(x=Xtrain, y=Ytrain, batch_size=len(Xtrain)*0.2, subset=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 18: What about big data?\n",
    "\n",
    "Question 24: How would you change the code for the image generator if you cannot fit all training images in CPU memory? What is the disadvantage of doing that change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Q24\n",
    "\n",
    "`Q24:` We can generate the images in batches i.e. reduce the batch_size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some augmented images\n",
    "plot_datagen = datagen.flow(Xtrain, Ytrain, batch_size=1)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "for i in range(18):\n",
    "    (im, label) = plot_datagen.next()\n",
    "    im = (im[0] + 1) * 127.5\n",
    "    im = im.astype('int')\n",
    "    label = np.flatnonzero(label)[0]\n",
    "    \n",
    "    plt.subplot(3,6,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(im)\n",
    "    plt.title(\"Class: {} ({})\".format(label, classes[label]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 19: Train the CNN with images from the generator\n",
    "\n",
    "See https://keras.io/models/model/ for how to use model.fit_generator instead of model.fit for training\n",
    "\n",
    "To make the comparison fair to training without augmentation\n",
    "\n",
    "    steps_per_epoch should be set to: len(Xtrain)*(1 - validation_split)/batch_size\n",
    "\n",
    "    validation_steps should be set to: len(Xtrain)*validation_split/batch_size\n",
    "\n",
    "Question 25: How quickly is the training accuracy increasing compared to without augmentation? Explain why there is a difference compared to without augmentation. What parameter is necessary to change to perform more training?\n",
    "\n",
    "Question 26: What other types of image augmentation can be applied, compared to what we use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers Q25-Q26\n",
    "\n",
    "`Q25:`\n",
    "    \n",
    "`Q26:` Images can also be cropped, translated (shifted left, right, up or down) and images can also be mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Build model (your best config)\n",
    "model6 = build_CNN(input_shape=input_shape, n_conv_layers=4, n_filters=16, n_dense_layers=1, n_nodes=80, \n",
    "                   use_dropout=True, learning_rate=0.01)\n",
    "\n",
    "validation_split=0.2\n",
    "    \n",
    "# Train the model using on the fly augmentation\n",
    "datagen_train = datagen.flow(x=Xtrain, y=Ytrain, batch_size=batch_size, subset=\"training\")\n",
    "datagen_valid = datagen.flow(x=Xtrain, y=Ytrain, batch_size=batch_size, subset=\"validation\")\n",
    "\n",
    "history6 = model6.fit_generator(generator=datagen_train, steps_per_epoch=len(Xtrain)*0.8/batch_size, epochs=epochs,\n",
    "                                validation_data=datagen_valid, validation_steps=len(Xtrain)*0.2/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is still a big difference in accuracy for original and rotated test images\n",
    "\n",
    "# Evaluate the trained model on original test set\n",
    "score = model6.evaluate(Xtest, Ytest, batch_size = batch_size, verbose=0)\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "# Evaluate the trained model on rotated test set\n",
    "score = model6.evaluate(Xtest_rotated, Ytest, batch_size = batch_size, verbose=0)\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 20: Plot misclassified images\n",
    "\n",
    "Lets plot some images where the CNN performed badly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified images\n",
    "y_pred = model6.predict_classes(Xtest)\n",
    "y_correct = np.argmax(Ytest,axis=1)\n",
    "\n",
    "miss = np.flatnonzero(y_correct != y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few of them\n",
    "plt.figure(figsize=(15,4))\n",
    "perm = np.random.permutation(miss)\n",
    "for i in range(18):\n",
    "    im = (Xtest[perm[i]] + 1) * 127.5\n",
    "    im = im.astype('int')\n",
    "    label_correct = y_correct[perm[i]]\n",
    "    label_pred = y_pred[perm[i]]\n",
    "    \n",
    "    plt.subplot(3,6,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"{}, classified as {}\".format(classes[label_correct], classes[label_pred]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 21: Testing on another size\n",
    "\n",
    "Question 27: This CNN has been trained on 32 x 32 images, can it be applied to images of another size? If not, why is this the case?\n",
    "\n",
    "Question 28: Is it possible to design a CNN that can be trained on images of one size, and then applied to an image of any size? How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers Q27-Q28\n",
    "\n",
    "`Q27:` No, our CNN trained on 32 x 32 images cannot be applied to images of another size because it is not a fully convolutional network. We have intermediate dense layers. In case of a dense layer, its parameters are predefined. Hence the input dimensions which goes to the dense layer are fixed. \n",
    "\n",
    "`Q28` Yes, a fully convolutional network can be trained on images of size A and then applied on images of size B. A fully convolutional network contains only convolutional layers and does not have any dense layers. Instead it has 1 x 1 convolutions that perform the task of fully connected layers. The input to the 1 x 1 convolution layer could be anything as long as the number of filters in 1 x 1 convolution layer (i.e. input to the final dense layer) is equal to the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 22: Pre-trained 2D CNNs\n",
    "\n",
    "There are many deep 2D CNNs that have been pre-trained using the large ImageNet database. Import a pre-trained ResNet50 network from Keras applications. Show the network using `model.summary()`\n",
    "\n",
    "Question 29: How many convolutional layers does ResNet50 have? \n",
    "\n",
    "Question 30: How many trainable parameters does the ResNet50 network have? \n",
    "\n",
    "Question 31: What is the size of the images that ResNet50 expects as input?\n",
    "\n",
    "Question 32: Using the answer to question 30, explain why the second derivative is seldom used when training deep networks.\n",
    "\n",
    "Apply the pre-trained CNN to 5 random color images that you download and copy to the cloud machine. Are the predictions correct? How certain is the network of each image class?\n",
    "\n",
    "These pre-trained networks can be fine tuned to your specific data, and normally only the last layers need to be re-trained, but it will still be too time consuming to do in this laboration.\n",
    "\n",
    "See https://keras.io/applications/#resnet\n",
    "\n",
    "Useful functions\n",
    "\n",
    "`image.load_img` in keras.preprocessing\n",
    "\n",
    "`image.img_to_array` in keras.preprocessing\n",
    "\n",
    "`ResNet50` in keras.applications.resnet50\n",
    "\n",
    "`preprocess_input` in keras.applications.resnet50\n",
    "\n",
    "`decode_predictions` in keras.applications.resnet50\n",
    "\n",
    "`expand_dims` in numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers Q29-Q32\n",
    "\n",
    "`Q29:`\n",
    "\n",
    "`Q30:`\n",
    "\n",
    "`Q31:`\n",
    "\n",
    "`Q32:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for using pre-trained ResNet 50 on 5 color images of your choice\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing import image\n",
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50(include_top=True, weights='imagenet')\n",
    "\n",
    "img_path = \n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(load_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
